{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlialin/miniconda3/envs/ldm/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/vlialin/miniconda3/envs/ldm/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# install sacrebleu, datasets, evaluate\n",
    "\n",
    "import time\n",
    "from glob import iglob\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import evaluate\n",
    "import easyocr\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "reader = easyocr.Reader([\"en\"], detect_network='craft', gpu=True)\n",
    "chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e12254b61b4bc2810cd745a8b82753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec: 7.533760620233949\n"
     ]
    }
   ],
   "source": [
    "# Do OCR\n",
    "image_paths = iglob(\"/home/vlialin/data/TextCaps/train/*\")\n",
    "\n",
    "n = 50\n",
    "start = time.time()\n",
    "for i, image_path in tqdm(enumerate(image_paths), total=n):\n",
    "    if i > n:\n",
    "        break\n",
    "    result = reader.readtext(\n",
    "        image_path,\n",
    "        batch_size=32,  # 6-8 examples/sec, GPU is unused most of the time\n",
    "        # workers=8,  # 1.12 examples/sec\n",
    "    )\n",
    "\n",
    "print(f\"Examples/sec: {n / (time.time() - start)}\")\n",
    "\n",
    "# Batched is much slower than unbatched, 5.0 examples/sec, GPU is unused most of the time\n",
    "# batch = []\n",
    "# batch_size = 32\n",
    "# n = batch_size * 10\n",
    "# start = time.time()\n",
    "# for i, image_path in tqdm(enumerate(image_paths), total=n):\n",
    "#     if i >= n:\n",
    "#         break\n",
    "#     batch.append(image_path)\n",
    "#     if len(batch) == batch_size:\n",
    "#         result = reader.readtext_batched(\n",
    "#             batch,\n",
    "#             # batch_size=batch_size,\n",
    "#             # workers=8,\n",
    "#             n_width=800, n_height=600\n",
    "#         )\n",
    "#         batch = []\n",
    "\n",
    "# print(n / (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watermarks = load_dataset(\"laion/laion2B-en-watermark\")\n",
    "# watermarks = watermarks.filter(lambda x: x[\"pwatermark\"] > 0.5)\n",
    "# watermars = dict(watermarks.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration laion--laion2B-en-joined-bc573946f750094d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download the dataset\n",
    "dataset = load_dataset(\"laion/laion2B-en-joined\", split=\"train\", streaming=True)\n",
    "dataset = dataset.shuffle(seed=834, buffer_size=10_000)\n",
    "\n",
    "# dataset example:\n",
    "# item: {'SAMPLE_ID': 2641080021034,\n",
    "#  'URL': 'https://cdn.shopify.com/s/files/1/0017/3621/2538/products/blue-beach-umbrellas-point-of-rocks-crescent-beach-siesta-key-shawn-mcloughlin_32d72f5b-5e55-42f9-bfcf-d6fa8d239beb_300x300.jpg?v=1524171284',\n",
    "#  'TEXT': 'Blue Beach Umbrellas, Point Of Rocks, Crescent Beach, Siesta Key - Spiral Notebook',\n",
    "#  'HEIGHT': 231,\n",
    "#  'WIDTH': 300,\n",
    "#  'LICENSE': '?',\n",
    "#  'NSFW': 'UNLIKELY',\n",
    "#  'similarity': 0.3955616354942322}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL': 'https://molinoproperty.com/wp-content/uploads/2017/06/29-4.jpg',\n",
       " 'TEXT': 'Villa in Valtocado - Mijas for sale',\n",
       " 'WIDTH': 1000,\n",
       " 'HEIGHT': 667,\n",
       " 'similarity': 0.3118513226509094,\n",
       " 'hash': 7507156594793326694,\n",
       " 'punsafe': 0.0034678280353546143,\n",
       " 'pwatermark': 0.02891501970589161}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c5015091d54e4f82578662daa73f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imgage to PIL\n",
      "Starting OCR\n",
      "Examples/sec: 1.4908328861052205\n",
      "Errors: 160\n",
      "Have OCR: 0.201\n"
     ]
    }
   ],
   "source": [
    "WATERMARKS = {\"gettyimages\"}\n",
    "\n",
    "n = 1000\n",
    "h_have_ocr = 0\n",
    "n_errors = 0\n",
    "\n",
    "start = time.time()\n",
    "for i, item in tqdm(enumerate(dataset), total=n):\n",
    "    if i >= n:\n",
    "        break\n",
    "\n",
    "    if item[\"punsafe\"] > 0.5:\n",
    "        continue\n",
    "\n",
    "    if item[\"pwatermark\"] > 0.5:\n",
    "        continue\n",
    "\n",
    "    if \"porn\" in item[\"TEXT\"].lower():\n",
    "        continue\n",
    "\n",
    "    # download image from URL without causing an error, set timeout to 1 second\n",
    "\n",
    "    try:\n",
    "        response = requests.get(item[\"URL\"], timeout=1)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        n_errors += 1\n",
    "        continue\n",
    "    if response.status_code != 200:\n",
    "        n_errors += 1\n",
    "        continue\n",
    "\n",
    "    if i == 42:\n",
    "        print(f\"Loading imgage to PIL\")\n",
    "\n",
    "    try:\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    except OSError as e:\n",
    "        n_errors += 1\n",
    "        continue\n",
    "\n",
    "    # image to numpy\n",
    "    image = np.array(image)\n",
    "\n",
    "    if i == 42:\n",
    "        print(f\"Starting OCR\")\n",
    "\n",
    "    # do OCR\n",
    "    ocr_results = reader.readtext(\n",
    "        image,\n",
    "        batch_size=32,  # 6.3 examples/sec, GPU is unused most of the time\n",
    "        # workers=8,\n",
    "    )\n",
    "\n",
    "    for ocr_item in ocr_results:\n",
    "        ocr_text = ocr_item[1].lower()\n",
    "        item_text = item[\"TEXT\"].lower()\n",
    "        ocr_confidence = ocr_item[2]\n",
    "\n",
    "        for w in WATERMARKS:\n",
    "            if w in ocr_text:\n",
    "                continue\n",
    "\n",
    "        if ocr_confidence < 0.8:\n",
    "            continue\n",
    "\n",
    "        similarity = chrf.compute(predictions=[ocr_text], references=[item_text])[\"score\"]\n",
    "        if similarity > 0.8:  # 0.8 is good\n",
    "            h_have_ocr += 1\n",
    "            break\n",
    "\n",
    "        #     print(f\"Image number: {i}\")\n",
    "        #     print(f\"Image url: {item['URL']}\")\n",
    "        #     print(f\"OCR: `{ocr_text}`, confidence: {ocr_confidence}, similarity: {similarity}\")\n",
    "        #     print(f\"TEXT: {item['TEXT']}\")\n",
    "        #     print()\n",
    "\n",
    "print(f\"Examples/sec: {n / (time.time() - start)}\")\n",
    "print(f\"Errors: {n_errors}\")\n",
    "print(f\"Have OCR: {h_have_ocr/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration laion--laion2B-en-joined-bc573946f750094d\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"laion/laion2B-en-joined\", split=\"train\", streaming=True)\n",
    "dataset = dataset.shuffle(seed=84, buffer_size=10_000)\n",
    "dataset = dataset.take(20_000_000)\n",
    "dataset = dataset.filter(lambda x: x[\"punsafe\"] is not None and x[\"punsafe\"] < 0.5 and x[\"pwatermark\"] is not None and x[\"pwatermark\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 16s, sys: 10.2 s, total: 3min 26s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_list = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "safe_dataset = Dataset.from_list(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['URL', 'TEXT', 'WIDTH', 'HEIGHT', 'similarity', 'hash', 'punsafe', 'pwatermark'],\n",
       "    num_rows: 14501921\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_dataset.save_to_disk(\"/home/vlialin/data/text-laion-20M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the downloading of this file\n",
      "Sharding file number 1 of 1 called /home/vlialin/documents/random_notebooks/1000_urls.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sharded in 10 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:19, 13.99s/it]\n"
     ]
    }
   ],
   "source": [
    "_1000_urls = [item[\"URL\"] for item in dataset_list[:10_000]]\n",
    "\n",
    "# save to csv\n",
    "import csv\n",
    "with open(\"1000_urls.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"url\"])\n",
    "    for url in _1000_urls:\n",
    "        writer.writerow([url])\n",
    "    \n",
    "# with open(\"1000_urls.txt\", \"w\") as f:\n",
    "#     for url in _1000_urls:\n",
    "#         f.write(f\"{url}\\n\")\n",
    "\n",
    "from img2dataset import download\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "output_dir = os.path.abspath(\"1000_images\")\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "download(\n",
    "    processes_count=16,\n",
    "    thread_count=32,\n",
    "    url_list=\"1000_urls.csv\",\n",
    "    image_size=512,\n",
    "    resize_mode=\"keep_ratio\",\n",
    "    output_folder=output_dir,\n",
    "    output_format=\"files\",\n",
    "    input_format=\"csv\",\n",
    "    enable_wandb=True,\n",
    "    number_sample_per_shard=1000,\n",
    "    distributor=\"multiprocessing\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2dataset --url_list /home/vlialin/data/text--100k --input_format \"arrow\" \\\n",
    "        --url_col \"URL\" --caption_col \"TEXT\" --output_format webdataset \\\n",
    "        --output_folder text-laion7m-data --processes_count 16 --thread_count 128 --image_size 256 \\\n",
    "        --save_additional_columns \"['WIDTH', 'HElaionIGHT', 'similarity', 'hash', 'punsafe', 'pwatermark']\" --enable_wandb True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "n_errors = 0\n",
    "\n",
    "data = []\n",
    "\n",
    "start = time.time()\n",
    "for i, item in tqdm(enumerate(dataset), total=n):\n",
    "    if i >= n:\n",
    "        break\n",
    "\n",
    "    if item[\"punsafe\"] > 0.5:\n",
    "        continue\n",
    "\n",
    "    if item[\"pwatermark\"] > 0.5:\n",
    "        continue\n",
    "\n",
    "    if \"porn\" in item[\"TEXT\"].lower():\n",
    "        continue\n",
    "\n",
    "    data.append(item)\n",
    "\n",
    "\n",
    "print(f\"Examples/sec: {n / (time.time() - start)}\")\n",
    "print(f\"Errors: {n_errors}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b82deec8ef69ed6bf59cb496158b0666eb63a2fbfc73c051e571d4744dc37e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
